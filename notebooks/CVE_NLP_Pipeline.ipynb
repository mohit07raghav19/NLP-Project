{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642b0b3c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation\n",
    "\n",
    "First, we'll install all required dependencies. This cell checks if we're running in Google Colab and installs packages accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"üîç Environment: {'Google Colab' if IN_COLAB else 'Local Jupyter'}\")\n",
    "print(f\"üêç Python Version: {sys.version.split()[0]}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüì¶ Installing dependencies for Google Colab...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install -q spacy transformers torch pandas numpy matplotlib seaborn plotly tqdm requests beautifulsoup4 sqlalchemy python-dotenv pyyaml\n",
    "    \n",
    "    # Download spaCy model\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "    # Clone repository (if needed)\n",
    "    if not Path('/content/NLP-project').exists():\n",
    "        print(\"\\nüì• Cloning repository...\")\n",
    "        !git clone https://github.com/mohit07raghav19/NLP-Project.git /content/NLP-project\n",
    "        os.chdir('/content/NLP-project')\n",
    "    else:\n",
    "        os.chdir('/content/NLP-project')\n",
    "else:\n",
    "    print(\"\\n‚úÖ Running locally - assuming dependencies are already installed\")\n",
    "    print(\"   If not, run: pip install -r requirements.txt\")\n",
    "    \n",
    "    # Navigate to project root\n",
    "    project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "    os.chdir(project_root)\n",
    "    print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b83cdb",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll be using throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# HTTP requests\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"ü§ñ spaCy available: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1c370",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up project configuration and API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'nvd_api_key': None,  # Set this to your NVD API key or leave None\n",
    "    'cache_enabled': True,\n",
    "    'rate_limit_delay': 0.6,  # seconds between requests\n",
    "    'batch_size': 100,\n",
    "    'max_cves_to_fetch': 1000,  # Limit for faster processing (set None for all)\n",
    "    'spacy_model': 'en_core_web_sm',\n",
    "    'database_url': 'sqlite:///data/cve_database.db'\n",
    "}\n",
    "\n",
    "# Create data directories\n",
    "for directory in ['data/raw', 'data/processed', 'data/cache', 'data/visualizations']:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüí° Tip: Get a free NVD API key at: https://nvd.nist.gov/developers/request-an-api-key\")\n",
    "print(\"   This increases rate limit from 5 to 50 requests per 30 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273844a",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Collection\n",
    "\n",
    "We'll fetch CVE data from the NVD API using our custom client that handles rate limiting and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom NVD client\n",
    "sys.path.insert(0, 'src')\n",
    "from data_collection.nvd_client import NVDClient\n",
    "\n",
    "# Initialize client\n",
    "nvd_client = NVDClient(\n",
    "    api_key=CONFIG['nvd_api_key'],\n",
    "    cache_enabled=CONFIG['cache_enabled']\n",
    ")\n",
    "\n",
    "print(\"üîå NVD API Client initialized\")\n",
    "print(f\"üì° Rate Limit: {nvd_client.rate_limit} requests per 30 seconds\")\n",
    "print(f\"üíæ Cache: {'Enabled' if nvd_client.cache_enabled else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch recent CVEs\n",
    "print(\"üì• Fetching CVE data from NVD API...\")\n",
    "print(\"‚è±Ô∏è This may take a few minutes depending on the number of CVEs...\\n\")\n",
    "\n",
    "# Fetch CVEs from the last 30 days\n",
    "cves = nvd_client.get_recent_cves(\n",
    "    days=30,\n",
    "    limit=CONFIG['max_cves_to_fetch']\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "raw_data_file = f\"data/raw/cves_{timestamp}.json\"\n",
    "nvd_client.save_to_json(cves, raw_data_file)\n",
    "\n",
    "print(f\"\\n‚úÖ Fetched {len(cves)} CVEs\")\n",
    "print(f\"üíæ Saved to: {raw_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c109d1a",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Let's examine the structure of the fetched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32feb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a sample CVE\n",
    "if cves:\n",
    "    sample_cve = cves[0]\n",
    "    print(\"üìÑ Sample CVE Structure:\")\n",
    "    print(json.dumps(sample_cve, indent=2)[:1000], \"...\\n\")\n",
    "    \n",
    "    # Extract basic information\n",
    "    cve_data = sample_cve.get('cve', {})\n",
    "    cve_id = cve_data.get('id', 'N/A')\n",
    "    descriptions = cve_data.get('descriptions', [])\n",
    "    description = descriptions[0].get('value', 'N/A') if descriptions else 'N/A'\n",
    "    \n",
    "    print(f\"üÜî CVE ID: {cve_id}\")\n",
    "    print(f\"üìù Description: {description[:200]}...\")\n",
    "    \n",
    "    # CVSS Score\n",
    "    metrics = cve_data.get('metrics', {})\n",
    "    cvss_v3 = metrics.get('cvssMetricV31', [{}])[0] if metrics.get('cvssMetricV31') else {}\n",
    "    if cvss_v3:\n",
    "        cvss_data = cvss_v3.get('cvssData', {})\n",
    "        print(f\"‚ö†Ô∏è CVSS Score: {cvss_data.get('baseScore', 'N/A')}\")\n",
    "        print(f\"üìä Severity: {cvss_data.get('baseSeverity', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CVEs fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac03ad",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Preprocessing\n",
    "\n",
    "Extract relevant fields and clean the text data for NLP processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec67503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cve_info(cve_entry: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured information from raw CVE entry.\n",
    "    \"\"\"\n",
    "    cve = cve_entry.get('cve', {})\n",
    "    \n",
    "    # Basic info\n",
    "    cve_id = cve.get('id', '')\n",
    "    \n",
    "    # Description\n",
    "    descriptions = cve.get('descriptions', [])\n",
    "    description = descriptions[0].get('value', '') if descriptions else ''\n",
    "    \n",
    "    # Dates\n",
    "    published = cve.get('published', '')\n",
    "    modified = cve.get('lastModified', '')\n",
    "    \n",
    "    # CVSS Metrics (try v3.1, then v3.0, then v2.0)\n",
    "    metrics = cve.get('metrics', {})\n",
    "    \n",
    "    cvss_v31 = metrics.get('cvssMetricV31', [{}])[0] if metrics.get('cvssMetricV31') else {}\n",
    "    cvss_v30 = metrics.get('cvssMetricV30', [{}])[0] if metrics.get('cvssMetricV30') else {}\n",
    "    cvss_v2 = metrics.get('cvssMetricV2', [{}])[0] if metrics.get('cvssMetricV2') else {}\n",
    "    \n",
    "    cvss_data = cvss_v31.get('cvssData', cvss_v30.get('cvssData', cvss_v2.get('cvssData', {})))\n",
    "    \n",
    "    # CWE (Common Weakness Enumeration)\n",
    "    weaknesses = cve.get('weaknesses', [])\n",
    "    cwe_ids = []\n",
    "    for weakness in weaknesses:\n",
    "        for desc in weakness.get('description', []):\n",
    "            cwe_ids.append(desc.get('value', ''))\n",
    "    \n",
    "    # References\n",
    "    references = cve.get('references', [])\n",
    "    reference_urls = [ref.get('url', '') for ref in references]\n",
    "    \n",
    "    return {\n",
    "        'cve_id': cve_id,\n",
    "        'description': description,\n",
    "        'published_date': published,\n",
    "        'last_modified_date': modified,\n",
    "        'cvss_version': cvss_data.get('version', ''),\n",
    "        'cvss_score': cvss_data.get('baseScore', 0.0),\n",
    "        'cvss_severity': cvss_data.get('baseSeverity', 'UNKNOWN'),\n",
    "        'cvss_vector': cvss_data.get('vectorString', ''),\n",
    "        'attack_vector': cvss_data.get('attackVector', ''),\n",
    "        'attack_complexity': cvss_data.get('attackComplexity', ''),\n",
    "        'cwe_ids': cwe_ids,\n",
    "        'references': reference_urls,\n",
    "        'raw_data': cve_entry\n",
    "    }\n",
    "\n",
    "# Extract information from all CVEs\n",
    "print(\"üîÑ Extracting structured information from CVEs...\")\n",
    "extracted_data = [extract_cve_info(cve) for cve in tqdm(cves, desc=\"Processing\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(extracted_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted {len(df)} CVE records\")\n",
    "print(f\"üìä Columns: {', '.join(df.columns[:8])}...\")\n",
    "print(f\"\\nüìà Data shape: {df.shape}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"üîç Data Quality Check:\\n\")\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Missing descriptions: {df['description'].isna().sum()}\")\n",
    "print(f\"Missing CVSS scores: {(df['cvss_score'] == 0).sum()}\")\n",
    "print(f\"Unique CVE IDs: {df['cve_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüìä Severity Distribution:\")\n",
    "print(df['cvss_severity'].value_counts())\n",
    "\n",
    "print(\"\\nüìÖ Date Range:\")\n",
    "df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "print(f\"From: {df['published_date'].min()}\")\n",
    "print(f\"To: {df['published_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4f4ed",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "\n",
    "Clean CVE descriptions for better NLP processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.cleaner import TextCleaner\n",
    "\n",
    "# Initialize text cleaner\n",
    "cleaner = TextCleaner(\n",
    "    lowercase=False,  # Keep case for entity recognition\n",
    "    remove_html=True,\n",
    "    remove_urls=False,  # Keep URLs as they might contain product info\n",
    "    preserve_cve_ids=True,\n",
    "    preserve_version_numbers=True\n",
    ")\n",
    "\n",
    "# Clean descriptions\n",
    "print(\"üßπ Cleaning text data...\")\n",
    "df['cleaned_description'] = df['description'].apply(cleaner.clean)\n",
    "\n",
    "# Show before/after example\n",
    "print(\"\\nüìù Cleaning Example:\")\n",
    "print(\"\\nBefore:\")\n",
    "print(df['description'].iloc[0][:300])\n",
    "print(\"\\nAfter:\")\n",
    "print(df['cleaned_description'].iloc[0][:300])\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaned {len(df)} descriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad86d2",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ NLP Processing\n",
    "\n",
    "Apply Named Entity Recognition and extract security-relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff4580",
   "metadata": {},
   "source": [
    "### Initialize NLP Models\n",
    "\n",
    "Load spaCy and prepare for entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be451728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.ner_extractor import NERExtractor\n",
    "\n",
    "# Initialize NER extractor\n",
    "print(\"ü§ñ Loading spaCy NLP model...\")\n",
    "ner_extractor = NERExtractor(model_name=CONFIG['spacy_model'])\n",
    "\n",
    "print(\"‚úÖ NER model loaded successfully!\")\n",
    "print(f\"üì¶ Model: {CONFIG['spacy_model']}\")\n",
    "print(f\"üè∑Ô∏è Available entity types: ORG, PRODUCT, PERSON, GPE, DATE, CARDINAL, VULN_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639df601",
   "metadata": {},
   "source": [
    "### Entity Extraction\n",
    "\n",
    "Extract named entities from CVE descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities from all descriptions\n",
    "print(\"üîç Extracting named entities...\")\n",
    "\n",
    "entities_list = []\n",
    "for desc in tqdm(df['cleaned_description'], desc=\"Extracting entities\"):\n",
    "    summary = ner_extractor.get_entity_summary(desc)\n",
    "    entities_list.append(summary)\n",
    "\n",
    "# Add to dataframe\n",
    "df['extracted_entities'] = entities_list\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted entities from {len(df)} CVEs\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nüìã Entity Extraction Example:\")\n",
    "print(f\"\\nText: {df['cleaned_description'].iloc[0][:200]}...\\n\")\n",
    "print(\"Extracted Entities:\")\n",
    "for entity_type, values in df['extracted_entities'].iloc[0].items():\n",
    "    if values and entity_type != 'all_entities':\n",
    "        print(f\"  {entity_type}: {values[:3]}\")  # Show first 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific entity types into columns\n",
    "df['vendors'] = df['extracted_entities'].apply(lambda x: x.get('organizations', []))\n",
    "df['products'] = df['extracted_entities'].apply(lambda x: x.get('products', []))\n",
    "df['vuln_types'] = df['extracted_entities'].apply(lambda x: x.get('vulnerability_types', []))\n",
    "\n",
    "print(\"üìä Entity Statistics:\\n\")\n",
    "print(f\"CVEs with vendor mentions: {(df['vendors'].str.len() > 0).sum()}\")\n",
    "print(f\"CVEs with product mentions: {(df['products'].str.len() > 0).sum()}\")\n",
    "print(f\"CVEs with vulnerability types: {(df['vuln_types'].str.len() > 0).sum()}\")\n",
    "\n",
    "# Most common vendors\n",
    "all_vendors = [vendor for vendors in df['vendors'] for vendor in vendors]\n",
    "print(f\"\\nüè¢ Top 10 Vendors:\")\n",
    "vendor_counts = Counter(all_vendors)\n",
    "for vendor, count in vendor_counts.most_common(10):\n",
    "    print(f\"  {vendor}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099b4f8",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Database Storage\n",
    "\n",
    "Store processed data in SQLite database for efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.models import Base, CVEModel\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Create database\n",
    "print(\"üíæ Setting up database...\")\n",
    "engine = create_engine(CONFIG['database_url'])\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "print(f\"‚úÖ Database created: {CONFIG['database_url']}\")\n",
    "print(f\"üìä Tables: {', '.join(Base.metadata.tables.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert CVEs into database\n",
    "print(f\"\\nüì• Inserting {len(df)} CVEs into database...\")\n",
    "\n",
    "inserted_count = 0\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Inserting\"):\n",
    "    try:\n",
    "        cve_model = CVEModel(\n",
    "            cve_id=row['cve_id'],\n",
    "            description=row['description'],\n",
    "            published_date=row['published_date'],\n",
    "            last_modified_date=pd.to_datetime(row['last_modified_date']),\n",
    "            cvss_version=row['cvss_version'],\n",
    "            cvss_score=row['cvss_score'],\n",
    "            cvss_severity=row['cvss_severity'],\n",
    "            cvss_vector=row['cvss_vector'],\n",
    "            attack_vector=row.get('attack_vector', ''),\n",
    "            attack_complexity=row.get('attack_complexity', ''),\n",
    "            affected_vendors=row['vendors'],\n",
    "            affected_products=row['products'],\n",
    "            vulnerability_types=row['vuln_types'],\n",
    "            extracted_entities=row['extracted_entities']\n",
    "        )\n",
    "        \n",
    "        # Check if exists\n",
    "        existing = session.query(CVEModel).filter_by(cve_id=row['cve_id']).first()\n",
    "        if not existing:\n",
    "            session.add(cve_model)\n",
    "            inserted_count += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting {row['cve_id']}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Commit changes\n",
    "session.commit()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {inserted_count} new CVEs\")\n",
    "print(f\"üìä Total CVEs in database: {session.query(CVEModel).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce026f94",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Analysis & Visualization\n",
    "\n",
    "Generate insights and visualizations from the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a287e37",
   "metadata": {},
   "source": [
    "### Severity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity distribution\n",
    "fig = px.pie(\n",
    "    df,\n",
    "    names='cvss_severity',\n",
    "    title='CVE Severity Distribution',\n",
    "    color_discrete_sequence=px.colors.sequential.RdBu\n",
    ")\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()\n",
    "\n",
    "# Bar chart with counts\n",
    "severity_counts = df['cvss_severity'].value_counts()\n",
    "fig2 = px.bar(\n",
    "    x=severity_counts.index,\n",
    "    y=severity_counts.values,\n",
    "    title='CVE Count by Severity',\n",
    "    labels={'x': 'Severity', 'y': 'Count'},\n",
    "    color=severity_counts.values,\n",
    "    color_continuous_scale='Reds'\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665908f",
   "metadata": {},
   "source": [
    "### Temporal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVEs over time\n",
    "df_time = df.copy()\n",
    "df_time['date'] = pd.to_datetime(df_time['published_date']).dt.date\n",
    "daily_counts = df_time.groupby('date').size().reset_index(name='count')\n",
    "\n",
    "fig = px.line(\n",
    "    daily_counts,\n",
    "    x='date',\n",
    "    y='count',\n",
    "    title='CVEs Published Over Time',\n",
    "    labels={'date': 'Date', 'count': 'Number of CVEs'}\n",
    ")\n",
    "fig.update_traces(line_color='#FF6B6B')\n",
    "fig.show()\n",
    "\n",
    "# Severity over time\n",
    "severity_time = df_time.groupby(['date', 'cvss_severity']).size().reset_index(name='count')\n",
    "fig2 = px.area(\n",
    "    severity_time,\n",
    "    x='date',\n",
    "    y='count',\n",
    "    color='cvss_severity',\n",
    "    title='CVE Severity Trends Over Time'\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93418370",
   "metadata": {},
   "source": [
    "### Vendor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top affected vendors\n",
    "all_vendors = [vendor for vendors in df['vendors'] for vendor in vendors if vendor]\n",
    "vendor_counts = Counter(all_vendors)\n",
    "\n",
    "top_vendors = pd.DataFrame(\n",
    "    vendor_counts.most_common(15),\n",
    "    columns=['Vendor', 'CVE Count']\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    top_vendors,\n",
    "    x='CVE Count',\n",
    "    y='Vendor',\n",
    "    orientation='h',\n",
    "    title='Top 15 Vendors by CVE Count',\n",
    "    color='CVE Count',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä Top 15 Vendors:\")\n",
    "print(top_vendors.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1ae6f",
   "metadata": {},
   "source": [
    "### CVSS Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVSS score histogram\n",
    "fig = px.histogram(\n",
    "    df[df['cvss_score'] > 0],\n",
    "    x='cvss_score',\n",
    "    nbins=50,\n",
    "    title='CVSS Score Distribution',\n",
    "    labels={'cvss_score': 'CVSS Score', 'count': 'Frequency'},\n",
    "    color_discrete_sequence=['#FF6B6B']\n",
    ")\n",
    "fig.add_vline(x=df['cvss_score'].mean(), line_dash=\"dash\", line_color=\"green\",\n",
    "              annotation_text=f\"Mean: {df['cvss_score'].mean():.2f}\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"üìä CVSS Statistics:\")\n",
    "print(f\"  Mean: {df['cvss_score'].mean():.2f}\")\n",
    "print(f\"  Median: {df['cvss_score'].median():.2f}\")\n",
    "print(f\"  Std Dev: {df['cvss_score'].std():.2f}\")\n",
    "print(f\"  Max: {df['cvss_score'].max():.2f}\")\n",
    "print(f\"  Min: {df[df['cvss_score'] > 0]['cvss_score'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495e1c9",
   "metadata": {},
   "source": [
    "### Vulnerability Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce131aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common vulnerability types\n",
    "all_vuln_types = [vtype for vtypes in df['vuln_types'] for vtype in vtypes if vtype]\n",
    "if all_vuln_types:\n",
    "    vuln_type_counts = Counter(all_vuln_types)\n",
    "    \n",
    "    top_vulns = pd.DataFrame(\n",
    "        vuln_type_counts.most_common(10),\n",
    "        columns=['Vulnerability Type', 'Count']\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        top_vulns,\n",
    "        x='Count',\n",
    "        y='Vulnerability Type',\n",
    "        orientation='h',\n",
    "        title='Top 10 Vulnerability Types',\n",
    "        color='Count',\n",
    "        color_continuous_scale='Reds'\n",
    "    )\n",
    "    fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No vulnerability types extracted (this is normal if not in descriptions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff534c6",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c596b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between attack vector and severity\n",
    "attack_severity = df[df['attack_vector'] != ''].groupby(['attack_vector', 'cvss_severity']).size().reset_index(name='count')\n",
    "\n",
    "if not attack_severity.empty:\n",
    "    fig = px.sunburst(\n",
    "        attack_severity,\n",
    "        path=['attack_vector', 'cvss_severity'],\n",
    "        values='count',\n",
    "        title='Attack Vector vs Severity'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Severity by attack complexity\n",
    "complexity_severity = df[df['attack_complexity'] != ''].groupby(['attack_complexity', 'cvss_severity']).size().reset_index(name='count')\n",
    "\n",
    "if not complexity_severity.empty:\n",
    "    fig2 = px.bar(\n",
    "        complexity_severity,\n",
    "        x='attack_complexity',\n",
    "        y='count',\n",
    "        color='cvss_severity',\n",
    "        title='Attack Complexity vs Severity',\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12d05e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluation & Metrics\n",
    "\n",
    "Calculate performance metrics for our NLP extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6bdb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate extraction metrics\n",
    "print(\"üìä NLP Extraction Metrics:\\n\")\n",
    "\n",
    "total_cves = len(df)\n",
    "\n",
    "# Entity extraction rates\n",
    "vendor_extraction_rate = (df['vendors'].str.len() > 0).sum() / total_cves * 100\n",
    "product_extraction_rate = (df['products'].str.len() > 0).sum() / total_cves * 100\n",
    "vuln_type_extraction_rate = (df['vuln_types'].str.len() > 0).sum() / total_cves * 100\n",
    "\n",
    "print(f\"Vendor Extraction Rate: {vendor_extraction_rate:.1f}%\")\n",
    "print(f\"Product Extraction Rate: {product_extraction_rate:.1f}%\")\n",
    "print(f\"Vulnerability Type Extraction Rate: {vuln_type_extraction_rate:.1f}%\")\n",
    "\n",
    "# Average entities per CVE\n",
    "avg_vendors = df['vendors'].str.len().mean()\n",
    "avg_products = df['products'].str.len().mean()\n",
    "\n",
    "print(f\"\\nAverage vendors per CVE: {avg_vendors:.2f}\")\n",
    "print(f\"Average products per CVE: {avg_products:.2f}\")\n",
    "\n",
    "# Coverage metrics\n",
    "print(f\"\\nCoverage Metrics:\")\n",
    "print(f\"  CVEs with CVSS scores: {(df['cvss_score'] > 0).sum()} ({(df['cvss_score'] > 0).sum()/total_cves*100:.1f}%)\")\n",
    "print(f\"  CVEs with CWE IDs: {(df['cwe_ids'].str.len() > 0).sum()} ({(df['cwe_ids'].str.len() > 0).sum()/total_cves*100:.1f}%)\")\n",
    "print(f\"  CVEs with references: {(df['references'].str.len() > 0).sum()} ({(df['references'].str.len() > 0).sum()/total_cves*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da891dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics visualization\n",
    "metrics_data = pd.DataFrame({\n",
    "    'Metric': ['Vendors', 'Products', 'Vuln Types', 'CVSS Score', 'CWE IDs'],\n",
    "    'Extraction Rate': [\n",
    "        vendor_extraction_rate,\n",
    "        product_extraction_rate,\n",
    "        vuln_type_extraction_rate,\n",
    "        (df['cvss_score'] > 0).sum() / total_cves * 100,\n",
    "        (df['cwe_ids'].str.len() > 0).sum() / total_cves * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    metrics_data,\n",
    "    x='Metric',\n",
    "    y='Extraction Rate',\n",
    "    title='Information Extraction Success Rates',\n",
    "    color='Extraction Rate',\n",
    "    color_continuous_scale='Greens',\n",
    "    text='Extraction Rate'\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\n",
    "fig.update_layout(yaxis_title='Success Rate (%)', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef4a74",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Export Results\n",
    "\n",
    "Save processed data and analysis results for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab80bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to various formats\n",
    "output_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. CSV export (full data)\n",
    "csv_file = f\"data/processed/cves_processed_{output_timestamp}.csv\"\n",
    "df_export = df.drop(columns=['raw_data', 'extracted_entities'])  # Remove complex objects\n",
    "df_export.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Exported to CSV: {csv_file}\")\n",
    "\n",
    "# 2. JSON export (with entities)\n",
    "json_file = f\"data/processed/cves_with_entities_{output_timestamp}.json\"\n",
    "df.to_json(json_file, orient='records', indent=2)\n",
    "print(f\"‚úÖ Exported to JSON: {json_file}\")\n",
    "\n",
    "# 3. Excel export (summary)\n",
    "excel_file = f\"data/processed/cves_summary_{output_timestamp}.xlsx\"\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    df_export.to_excel(writer, sheet_name='CVEs', index=False)\n",
    "    \n",
    "    # Add summary statistics sheet\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'Metric': ['Total CVEs', 'Critical', 'High', 'Medium', 'Low', 'Average CVSS Score'],\n",
    "        'Value': [\n",
    "            len(df),\n",
    "            (df['cvss_severity'] == 'CRITICAL').sum(),\n",
    "            (df['cvss_severity'] == 'HIGH').sum(),\n",
    "            (df['cvss_severity'] == 'MEDIUM').sum(),\n",
    "            (df['cvss_severity'] == 'LOW').sum(),\n",
    "            f\"{df['cvss_score'].mean():.2f}\"\n",
    "        ]\n",
    "    })\n",
    "    summary_stats.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(f\"‚úÖ Exported to Excel: {excel_file}\")\n",
    "\n",
    "# 4. Summary statistics\n",
    "stats_file = f\"data/processed/statistics_{output_timestamp}.txt\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    f.write(\"CVE NLP Analysis - Statistics Summary\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total CVEs Analyzed: {len(df)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Severity Distribution:\\n\")\n",
    "    for severity, count in df['cvss_severity'].value_counts().items():\n",
    "        f.write(f\"  {severity}: {count} ({count/len(df)*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nAverage CVSS Score: {df['cvss_score'].mean():.2f}\\n\")\n",
    "    f.write(f\"Median CVSS Score: {df['cvss_score'].median():.2f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTop 5 Vendors:\\n\")\n",
    "    for vendor, count in vendor_counts.most_common(5):\n",
    "        f.write(f\"  {vendor}: {count}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Exported statistics: {stats_file}\")\n",
    "\n",
    "print(f\"\\nüéâ All exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17aace8",
   "metadata": {},
   "source": [
    "## üéì Summary & Conclusions\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Data Collection**: Successfully fetched CVE data from NVD API with rate limiting\n",
    "2. **Preprocessing**: Cleaned and normalized text data while preserving important patterns\n",
    "3. **NLP Processing**: Extracted named entities using spaCy NER\n",
    "4. **Database Storage**: Stored structured data in SQLite for efficient querying\n",
    "5. **Analysis**: Generated comprehensive visualizations and insights\n",
    "6. **Evaluation**: Measured extraction accuracy and coverage\n",
    "7. **Export**: Saved results in multiple formats (CSV, JSON, Excel)\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Analyzed {len(df)} CVEs from the last 30 days\n",
    "- Severity distribution shows the current threat landscape\n",
    "- Top vendors and products most affected by vulnerabilities\n",
    "- NLP extraction achieved good coverage for organization and product names\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Improve Extraction**: Fine-tune NER model for better security term recognition\n",
    "2. **Add Transformers**: Implement BERT-based extraction for better accuracy\n",
    "3. **Build API**: Create REST API for programmatic access\n",
    "4. **Develop Dashboard**: Build interactive web dashboard\n",
    "5. **Automate**: Set up scheduled data collection and analysis\n",
    "6. **Predict**: Build ML models to predict vulnerability severity\n",
    "\n",
    "### üìö Learning Takeaways\n",
    "\n",
    "- Real-world NLP applications in cybersecurity\n",
    "- API integration and rate limiting best practices\n",
    "- Entity extraction techniques and challenges\n",
    "- Data visualization for security insights\n",
    "- End-to-end ML pipeline development\n",
    "\n",
    "---\n",
    "\n",
    "**Project Repository**: [https://github.com/mohit07raghav19/NLP-Project](https://github.com/mohit07raghav19/NLP-Project)\n",
    "\n",
    "**Author**: Mohit Raghav\n",
    "\n",
    "**Thank you for exploring this CVE NLP Analysis System!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
